{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b00c29bd",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eda6142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Audio\n",
    "from pyannote.core import Annotation, Segment\n",
    "from pyannote.metrics.diarization import DiarizationErrorRate\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7a354b",
   "metadata": {},
   "source": [
    "## Work on CAM++ Diarization file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "257bfe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read can_plus csv file\n",
    "can_plus = pd.read_csv('Diarization results/can_plus_diarized_output.csv')\n",
    "sortformer_diarization = pd.read_csv('Diarization results/sortformer_diarization.csv')\n",
    "\n",
    "# Add 'audio_keep' column to can_plus if 'audio_id' in can_plus exists in sortformer_diarization\n",
    "can_plus['audio_keep'] = can_plus['audio_id'].isin(sortformer_diarization['audio_id'])\n",
    "\n",
    "# Copy 'transcript' from sortformer_diarization to can_plus where 'audio_id' matches\n",
    "can_plus['transcript'] = can_plus['audio_id'].map(sortformer_diarization.set_index('audio_id')['transcript'])\n",
    "\n",
    "# Filter can_plus to keep only rows where 'audio_keep' is True\n",
    "can_plus = can_plus[can_plus['audio_keep']].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02c2a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the {'text': ...} wrapper if present in each pred_segment\n",
    "for i, row in can_plus.iterrows():\n",
    "    seg = row['pred_segment']\n",
    "    # If seg is a string, try to eval to dict/list\n",
    "    if isinstance(seg, str):\n",
    "        try:\n",
    "            seg = eval(seg)\n",
    "        except Exception:\n",
    "            seg = seg\n",
    "    # If seg is a dict with 'text' key, extract it\n",
    "    if isinstance(seg, dict) and 'text' in seg:\n",
    "        segments_list = seg['text']\n",
    "    elif isinstance(seg, list):\n",
    "        segments_list = seg\n",
    "    else:\n",
    "        segments_list = []\n",
    "    formatted_segments = []\n",
    "    for s in segments_list:\n",
    "        # If segment is a list/tuple of length 3, convert to tuple\n",
    "        if isinstance(s, (list, tuple)) and len(s) == 3:\n",
    "            start, end, speaker = s\n",
    "            # Convert numpy types to float/int/str\n",
    "            try:\n",
    "                start = float(start)\n",
    "                end = float(end)\n",
    "                speaker = str(speaker)\n",
    "            except Exception:\n",
    "                pass\n",
    "            formatted_segments.append((start, end, speaker))\n",
    "        # If segment is a string, try to split\n",
    "        elif isinstance(s, str):\n",
    "            parts = s.split()\n",
    "            if len(parts) >= 3:\n",
    "                formatted_segments.append((float(parts[0]), float(parts[1]), parts[2]))\n",
    "    formatted_segments.sort(key=lambda x: x[0])\n",
    "    can_plus.at[i, 'pred_segment'] = formatted_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b97e663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time_to_seconds(timestamp):\n",
    "    # Split the timestamp into minutes, seconds, and milliseconds\n",
    "    minutes, seconds, milliseconds = map(float, timestamp.split(':'))\n",
    "    # Convert the time to seconds (including fractional part from milliseconds)\n",
    "    total_seconds = minutes * 60 + seconds + milliseconds / 1000\n",
    "    return total_seconds\n",
    "\n",
    "\n",
    "def extract_segments(transcript):\n",
    "    # Regular expression to match the timestamp and speaker tag\n",
    "    timestamp_pattern = r'(\\d{2}:\\d{2}:\\d{2})'\n",
    "    speaker_pattern = r'\\[([^\\]]+)\\]'\n",
    "\n",
    "    lines = transcript.strip().splitlines()\n",
    "    segments = []\n",
    "\n",
    "    start_time = None\n",
    "    speaker_tag = None\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        if re.match(timestamp_pattern, lines[i]):  # Line is a timestamp\n",
    "            if start_time and speaker_tag:\n",
    "                # If we have both start and speaker, the current timestamp is the end time\n",
    "                end_time = convert_time_to_seconds(lines[i])\n",
    "                segments.append((start_time, end_time, speaker_tag))\n",
    "                start_time = None\n",
    "                speaker_tag = None\n",
    "            # Set the new start time, converting to seconds\n",
    "            start_time = convert_time_to_seconds(lines[i])\n",
    "        elif re.match(speaker_pattern, lines[i]):  # Line contains a speaker tag\n",
    "            speaker_tag = re.findall(speaker_pattern, lines[i])[0]\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d61a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure new line before speaker tags\n",
    "can_plus['transcript'] = can_plus['transcript'].apply(lambda x: str(x).replace('[', '\\r\\n['))\n",
    "can_plus['ref_segments'] = can_plus['transcript'].apply(lambda x: extract_segments(x))\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "can_plus.to_csv('/home/kelechi/Dialect-Classification/Diarization results/CAM_Plus_plus_diarization.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b95f42",
   "metadata": {},
   "source": [
    "## Read other model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b416890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv files\n",
    "assemblyai = pd.read_csv('Diarization results/assemblyai_diarization_der_0.1272_30.csv')\n",
    "deepgram = pd.read_csv('Diarization results/deepgram_diarization_der_0.1421_30.csv')\n",
    "sortformer = pd.read_csv('Diarization results/sortformer_diarization.csv')\n",
    "pyannote = pd.read_csv('Diarization results/pyannote_diarization_der_0.2130_30.csv')\n",
    "soniox = pd.read_csv('Diarization results/soniox_diarization_der_0.2005_30.csv')\n",
    "reverb = pd.read_csv('Diarization results/reverb_diarization_der_0.2687_30.csv')\n",
    "cam = pd.read_csv('Diarization results/CAM_Plus_plus_diarization.csv')\n",
    "\n",
    "# Select only rows [0:31] of reverb\n",
    "reverb = reverb.iloc[0:31]\n",
    "# Filter out rows where 'audio_id' is nan in reverb\n",
    "reverb = reverb[reverb['audio_id'].notna()]\n",
    "\n",
    "#Rename 'pred_segment' to 'pred_segments' in cam\n",
    "cam.rename(columns={'pred_segment': 'pred_segments'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efd0670",
   "metadata": {},
   "source": [
    "## Edit speaker tagging in pred_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c655207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def parse_segments(cell):\n",
    "    # If already a list of tuples, return as is\n",
    "    if isinstance(cell, list) and all(isinstance(x, tuple) for x in cell):\n",
    "        return cell\n",
    "    # If it's a list of single characters, join and eval\n",
    "    if isinstance(cell, list):\n",
    "        cell = ''.join(cell)\n",
    "    # If it's a string, eval\n",
    "    if isinstance(cell, str):\n",
    "        try:\n",
    "            return ast.literal_eval(cell)\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def map_speaker_labels(segments):\n",
    "    mapped = []\n",
    "    for seg in segments:\n",
    "        if len(seg) == 3:\n",
    "            start, end, speaker = seg\n",
    "            if speaker == '0':\n",
    "                speaker = 'Speaker A'\n",
    "            elif speaker == '1':\n",
    "                speaker = 'Speaker B'\n",
    "            mapped.append((start, end, speaker))\n",
    "        else:\n",
    "            mapped.append(seg)\n",
    "    return mapped\n",
    "\n",
    "# Apply both functions to the column\n",
    "cam['pred_segments'] = cam['pred_segments'].apply(parse_segments).apply(map_speaker_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2fa846",
   "metadata": {},
   "source": [
    "## Check if audio_id is same for all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7da10c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All audio IDs match across the dataframes.\n"
     ]
    }
   ],
   "source": [
    "# Verify that 'audio_id' columns are the same across all dataframes, if successful print a message\n",
    "def verify_audio_ids(*dfs):\n",
    "    audio_ids = [set(df['audio_id']) for df in dfs]\n",
    "    if not all(audio_ids[0] == audio_id for audio_id in audio_ids):\n",
    "        raise ValueError(\"Audio IDs do not match across all dataframes.\")\n",
    "        # Print all mismatched audio IDs\n",
    "    else:\n",
    "        print(\"All audio IDs match across the dataframes.\")\n",
    "verify_audio_ids(assemblyai, deepgram, sortformer, pyannote, soniox, reverb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5f7bdb",
   "metadata": {},
   "source": [
    "## DER Matrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eb09cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pyannote_annotation(segments_list):\n",
    "    annotation = Annotation()\n",
    "    for start, end, speaker_tag in segments_list:\n",
    "        segment = Segment(start, end)\n",
    "        annotation[segment] = speaker_tag\n",
    "    return annotation\n",
    "\n",
    "der_metric = DiarizationErrorRate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9f683c",
   "metadata": {},
   "source": [
    "## Calculate absolute DER for all domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b513973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing assemblyai (ALL DOMAIN)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/30 [00:00<?, ?it/s]/home/kelechi/miniconda3/envs/nemo310/lib/python3.10/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n",
      "Processing:   0%|          | 0/30 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'FA'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, df \u001b[38;5;129;01min\u001b[39;00m datasets\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (ALL DOMAIN)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m     der_df, abs_der \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_der_for_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     der_results_all[name] \u001b[38;5;241m=\u001b[39m der_df\n\u001b[1;32m     32\u001b[0m     abs_ders_all[name] \u001b[38;5;241m=\u001b[39m abs_der\n",
      "Cell \u001b[0;32mIn[24], line 10\u001b[0m, in \u001b[0;36mcompute_der_for_dataset\u001b[0;34m(df, ref_col, pred_col)\u001b[0m\n\u001b[1;32m      8\u001b[0m     pred_annotation \u001b[38;5;241m=\u001b[39m create_pyannote_annotation(\u001b[38;5;28meval\u001b[39m(row[pred_col]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row[pred_col], \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m row[pred_col])\n\u001b[1;32m      9\u001b[0m     der \u001b[38;5;241m=\u001b[39m der_metric(ref_annotation, pred_annotation, detailed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 10\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_id\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_id\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDER\u001b[39m\u001b[38;5;124m'\u001b[39m: der, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFalse Alarm\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mder\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissed Detection\u001b[39m\u001b[38;5;124m'\u001b[39m: der[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMISS\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfusion\u001b[39m\u001b[38;5;124m'\u001b[39m: der[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCONFUSION\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n\u001b[1;32m     11\u001b[0m abs_der \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(der_metric)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAbsolute DER for dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mabs_der\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'FA'"
     ]
    }
   ],
   "source": [
    "def compute_der_for_dataset(df, ref_col='ref_segments', pred_col='pred_segments'):\n",
    "    results = []\n",
    "    der_metric = DiarizationErrorRate()\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\"):\n",
    "        if not (isinstance(row[ref_col], (str, list)) and isinstance(row[pred_col], (str, list))):\n",
    "            continue\n",
    "        ref_annotation = create_pyannote_annotation(eval(row[ref_col]) if isinstance(row[ref_col], str) else row[ref_col])\n",
    "        pred_annotation = create_pyannote_annotation(eval(row[pred_col]) if isinstance(row[pred_col], str) else row[pred_col])\n",
    "        der = der_metric(ref_annotation, pred_annotation)\n",
    "        results.append({'audio_id': row['audio_id'], 'DER': der})\n",
    "    abs_der = abs(der_metric)\n",
    "    print(f\"Absolute DER for dataset: {100 * abs_der:.2f}%\")\n",
    "    return pd.DataFrame(results), abs_der\n",
    "\n",
    "datasets = {\n",
    "    'assemblyai': assemblyai,\n",
    "    'deepgram': deepgram,\n",
    "    'sortformer': sortformer,\n",
    "    'pyannote': pyannote,\n",
    "    'soniox': soniox,\n",
    "    'reverb': reverb,\n",
    "    'cam': cam,\n",
    "}\n",
    "\n",
    "der_results_all = {}\n",
    "abs_ders_all = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\nProcessing {name} (ALL DOMAIN)...\")\n",
    "    der_df, abs_der = compute_der_for_dataset(df)\n",
    "    der_results_all[name] = der_df\n",
    "    abs_ders_all[name] = abs_der\n",
    "\n",
    "abs_der_df_all = pd.DataFrame.from_dict(abs_ders_all, orient='index', columns=['Absolute DER (All Domain)']).reset_index().rename(columns={'index': 'model'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083d5fa0",
   "metadata": {},
   "source": [
    "## Absolute DER for Medical Domain Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bd84f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing assemblyai (MEDICAL DOMAIN)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/9 [00:00<?, ?it/s]/home/kelechi/miniconda3/envs/nemo310/lib/python3.10/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n",
      "Processing: 100%|██████████| 9/9 [00:00<00:00, 62.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute DER for dataset: 25.68%\n",
      "\n",
      "Processing deepgram (MEDICAL DOMAIN)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/9 [00:00<?, ?it/s]/home/kelechi/miniconda3/envs/nemo310/lib/python3.10/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n",
      "Processing: 100%|██████████| 9/9 [00:00<00:00, 71.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute DER for dataset: 29.35%\n",
      "\n",
      "Processing sortformer (MEDICAL DOMAIN)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/9 [00:00<?, ?it/s]/home/kelechi/miniconda3/envs/nemo310/lib/python3.10/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n",
      "Processing: 100%|██████████| 9/9 [00:00<00:00, 33.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute DER for dataset: 39.64%\n",
      "\n",
      "Processing pyannote (MEDICAL DOMAIN)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/9 [00:00<?, ?it/s]/home/kelechi/miniconda3/envs/nemo310/lib/python3.10/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n",
      "Processing: 100%|██████████| 9/9 [00:00<00:00, 78.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute DER for dataset: 31.46%\n",
      "\n",
      "Processing soniox (MEDICAL DOMAIN)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/9 [00:00<?, ?it/s]/home/kelechi/miniconda3/envs/nemo310/lib/python3.10/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n",
      "Processing: 100%|██████████| 9/9 [00:00<00:00, 87.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute DER for dataset: 42.16%\n",
      "\n",
      "Processing reverb (MEDICAL DOMAIN)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/9 [00:00<?, ?it/s]/home/kelechi/miniconda3/envs/nemo310/lib/python3.10/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n",
      "Processing: 100%|██████████| 9/9 [00:00<00:00, 78.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute DER for dataset: 31.46%\n",
      "\n",
      "Processing cam (MEDICAL DOMAIN)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/9 [00:00<?, ?it/s]/home/kelechi/miniconda3/envs/nemo310/lib/python3.10/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n",
      "Processing: 100%|██████████| 9/9 [00:00<00:00, 66.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute DER for dataset: 34.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def filter_by_domain(df, domain='OSCE-Doctor-Patient'):\n",
    "    return df[df['domain'] == domain].reset_index(drop=True)\n",
    "\n",
    "filtered_datasets_medical = {name: filter_by_domain(df) for name, df in datasets.items()}\n",
    "\n",
    "der_results_medical = {}\n",
    "abs_ders_medical = {}\n",
    "medical_audio = []\n",
    "\n",
    "for name, df in filtered_datasets_medical.items():\n",
    "    print(f\"\\nProcessing {name} (MEDICAL DOMAIN)...\")\n",
    "    der_df, abs_der = compute_der_for_dataset(df)\n",
    "    der_df['model'] = name\n",
    "    medical_audio.append(der_df)\n",
    "    der_results_medical[name] = der_df\n",
    "    abs_ders_medical[name] = abs_der\n",
    "\n",
    "medical_audio = pd.concat(medical_audio, ignore_index=True)\n",
    "abs_der_df_medical = pd.DataFrame.from_dict(abs_ders_medical, orient='index', columns=['Medical Absolute DER']).reset_index().rename(columns={'index': 'model'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ad020",
   "metadata": {},
   "source": [
    "## Absolute DER for Non-Medical Domain Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b267b634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing assemblyai (NON-MEDICAL DOMAIN)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/21 [00:00<?, ?it/s]/home/kelechi/miniconda3/envs/nemo310/lib/python3.10/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n",
      "Processing: 100%|██████████| 21/21 [00:00<00:00, 235.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute DER for dataset: 9.91%\n",
      "\n",
      "Processing deepgram (NON-MEDICAL DOMAIN)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/21 [00:00<?, ?it/s]/home/kelechi/miniconda3/envs/nemo310/lib/python3.10/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n",
      "Processing: 100%|██████████| 21/21 [00:00<00:00, 206.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute DER for dataset: 10.92%\n",
      "\n",
      "Processing sortformer (NON-MEDICAL DOMAIN)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/21 [00:00<?, ?it/s]/home/kelechi/miniconda3/envs/nemo310/lib/python3.10/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n",
      "Processing: 100%|██████████| 21/21 [00:00<00:00, 41.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute DER for dataset: 24.04%\n",
      "\n",
      "Processing pyannote (NON-MEDICAL DOMAIN)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/21 [00:00<?, ?it/s]/home/kelechi/miniconda3/envs/nemo310/lib/python3.10/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n",
      "Processing: 100%|██████████| 21/21 [00:00<00:00, 230.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute DER for dataset: 19.09%\n",
      "\n",
      "Processing soniox (NON-MEDICAL DOMAIN)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/21 [00:00<?, ?it/s]/home/kelechi/miniconda3/envs/nemo310/lib/python3.10/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n",
      "Processing: 100%|██████████| 21/21 [00:00<00:00, 327.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute DER for dataset: 15.24%\n",
      "\n",
      "Processing reverb (NON-MEDICAL DOMAIN)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/21 [00:00<?, ?it/s]/home/kelechi/miniconda3/envs/nemo310/lib/python3.10/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n",
      "Processing: 100%|██████████| 21/21 [00:00<00:00, 237.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute DER for dataset: 17.68%\n",
      "\n",
      "Processing cam (NON-MEDICAL DOMAIN)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/21 [00:00<?, ?it/s]/home/kelechi/miniconda3/envs/nemo310/lib/python3.10/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n",
      "Processing: 100%|██████████| 21/21 [00:00<00:00, 159.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute DER for dataset: 16.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter all datasets for 'Chit-Chat-NG' domain\n",
    "def filter_by_domain(df, domain='OSCE-Doctor-Patient'):\n",
    "    return df[df['domain'] != domain].reset_index(drop=True)\n",
    "\n",
    "filtered_datasets_non_medical = {name: filter_by_domain(df) for name, df in datasets.items()}\n",
    "\n",
    "der_results_non_medical = {}\n",
    "abs_ders_non_medical = {}\n",
    "non_medical_audio = []\n",
    "\n",
    "for name, df in filtered_datasets_non_medical.items():\n",
    "    print(f\"\\nProcessing {name} (NON-MEDICAL DOMAIN)...\")\n",
    "    der_df, abs_der = compute_der_for_dataset(df)\n",
    "    der_df['model'] = name\n",
    "    non_medical_audio.append(der_df)\n",
    "    der_results_non_medical[name] = der_df\n",
    "    abs_ders_non_medical[name] = abs_der\n",
    "\n",
    "non_medical_audio = pd.concat(non_medical_audio, ignore_index=True)\n",
    "abs_der_df_non_medical = pd.DataFrame.from_dict(abs_ders_non_medical, orient='index', columns=['Non-Medical Absolute DER']).reset_index().rename(columns={'index': 'model'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c0902b",
   "metadata": {},
   "source": [
    "## View all doamin, medical doamain and general domain files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae6f89cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All Absolute DERs across domains:\n",
      "        model  Absolute DER (All Domain)  Medical Absolute DER  \\\n",
      "0  assemblyai                   0.127220              0.256756   \n",
      "1         cam                   0.195766              0.346428   \n",
      "2    deepgram                   0.142089              0.293510   \n",
      "3    pyannote                   0.213007              0.314621   \n",
      "4      reverb                   0.202347              0.314621   \n",
      "5      soniox                   0.200471              0.421604   \n",
      "6  sortformer                   0.268240              0.396392   \n",
      "\n",
      "   Non-Medical Absolute DER  \n",
      "0                  0.099077  \n",
      "1                  0.163033  \n",
      "2                  0.109191  \n",
      "3                  0.190930  \n",
      "4                  0.176771  \n",
      "5                  0.152428  \n",
      "6                  0.240398  \n"
     ]
    }
   ],
   "source": [
    "# --- DISPLAY ---\n",
    "# print(\"\\nAbsolute DERs for ALL DOMAIN datasets:\")\n",
    "# print(abs_der_df_all)\n",
    "\n",
    "# print(\"\\nAbsolute DERs for MEDICAL DOMAIN datasets:\")\n",
    "# print(abs_der_df_medical)\n",
    "\n",
    "# print(\"\\nAbsolute DERs for NON-MEDICAL DOMAIN datasets:\")\n",
    "# print(abs_der_df_non_medical)\n",
    "\n",
    "# Print a joint DataFrame with all absolute DERs, including all domains\n",
    "all_abs_der_df = pd.merge(abs_der_df_all, abs_der_df_medical, on='model', how='outer')\n",
    "all_abs_der_df = pd.merge(all_abs_der_df, abs_der_df_non_medical, on='model', how='outer')\n",
    "print(\"\\nAll Absolute DERs across domains:\")\n",
    "print(all_abs_der_df)\n",
    "\n",
    "# Save all absolute DERs to a CSV file\n",
    "all_abs_der_df.to_csv('Diarization results/absolute_der_all_domains_and_models.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b70e9b",
   "metadata": {},
   "source": [
    "## Calculate Strict DER for all domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea141f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_strict_der_for_dataset(df, ref_col='ref_segments', pred_col='pred_segments'):\n",
    "    results = []\n",
    "    # Initialize DiarizationErrorRate with strict parameters\n",
    "    der_metric = DiarizationErrorRate(collar=0.0, skip_overlap=True)\n",
    "\n",
    "    # Initialize accumulators for false alarm, missed detection, confusion, and total reference duration\n",
    "    total_fa = 0.0\n",
    "    total_miss = 0.0\n",
    "    total_confusion = 0.0\n",
    "    total_ref_duration = 0.0\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\"):\n",
    "        if not (isinstance(row[ref_col], (str, list)) and isinstance(row[pred_col], (str, list))):\n",
    "            continue\n",
    "        # Create pyannote annotations for reference and prediction\n",
    "        ref_annotation = create_pyannote_annotation(eval(row[ref_col]) if isinstance(row[ref_col], str) else row[ref_col])\n",
    "        pred_annotation = create_pyannote_annotation(eval(row[pred_col]) if isinstance(row[pred_col], str) else row[pred_col])\n",
    "        \n",
    "        # Compute DER for the current row\n",
    "        der = der_metric(ref_annotation, pred_annotation, detailed=True)\n",
    "        results.append({'audio_id': row['audio_id'], 'DER': der['diarization error rate']})\n",
    "        \n",
    "        # Compute components for the current row\n",
    "        components = der_metric.compute_components(ref_annotation, pred_annotation)\n",
    "        total_fa += components['false alarm']\n",
    "        total_miss += components['missed detection']\n",
    "        total_confusion += components['confusion']\n",
    "        total_ref_duration += components['total']\n",
    "\n",
    "    # Normalize metrics by total reference duration to express them as rates\n",
    "    fa_rate = total_fa / total_ref_duration if total_ref_duration > 0 else 0.0\n",
    "    miss_rate = total_miss / total_ref_duration if total_ref_duration > 0 else 0.0\n",
    "    confusion_rate = total_confusion / total_ref_duration if total_ref_duration > 0 else 0.0\n",
    "\n",
    "    # Compute the absolute DER for the entire dataset\n",
    "    abs_der = abs(der_metric)\n",
    "\n",
    "    print(f\"Strict Absolute DER for dataset: {100 * abs_der:.2f}%\")\n",
    "    # print(f\"False Alarm Rate: {100 * fa_rate:.2f}%\")\n",
    "    # print(f\"Missed Detection Rate: {100 * miss_rate:.2f}%\")\n",
    "    # print(f\"Confusion Rate: {100 * confusion_rate:.2f}%\")\n",
    "\n",
    "    return pd.DataFrame(results), abs_der, fa_rate, miss_rate, confusion_rate\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    'assemblyai': assemblyai,\n",
    "    'deepgram': deepgram,\n",
    "    'sortformer': sortformer,\n",
    "    'pyannote': pyannote,\n",
    "    'soniox': soniox,\n",
    "    'reverb': reverb,\n",
    "    'cam': cam,\n",
    "}\n",
    "\n",
    "der_results_all = {}\n",
    "abs_ders_all = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\nProcessing {name} (ALL DOMAIN)...\")\n",
    "    der_df, abs_der, fa_rate, miss_rate, confusion_rate = compute_strict_der_for_dataset(df)\n",
    "    der_results_all[name] = der_df\n",
    "    abs_ders_all[name] = {\n",
    "        'Absolute DER': abs_der,\n",
    "        'False Alarm Rate': fa_rate,\n",
    "        'Missed Detection Rate': miss_rate,\n",
    "        'Confusion Rate': confusion_rate\n",
    "    }\n",
    "\n",
    "# Create a DataFrame to summarize the results\n",
    "strict_der_df_all = pd.DataFrame.from_dict(abs_ders_all, orient='index').reset_index().rename(columns={'index': 'model'})\n",
    "\n",
    "print(\"\\nSummary of DER and accumulated metrics across all models:\")\n",
    "print(strict_der_df_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad11873e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing assemblyai (Medical Domain)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/9 [00:00<?, ?it/s]/home/kelechi/miniconda3/envs/nemo310/lib/python3.10/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n",
      "Processing: 100%|██████████| 9/9 [00:00<00:00, 28.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strict Absolute DER for dataset: 25.45%\n",
      "\n",
      "Processing deepgram (Medical Domain)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 9/9 [00:00<00:00, 33.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strict Absolute DER for dataset: 29.12%\n",
      "\n",
      "Processing sortformer (Medical Domain)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 9/9 [00:00<00:00, 16.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strict Absolute DER for dataset: 39.58%\n",
      "\n",
      "Processing pyannote (Medical Domain)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 9/9 [00:00<00:00, 36.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strict Absolute DER for dataset: 31.60%\n",
      "\n",
      "Processing soniox (Medical Domain)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 9/9 [00:00<00:00, 40.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strict Absolute DER for dataset: 42.10%\n",
      "\n",
      "Processing reverb (Medical Domain)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 9/9 [00:00<00:00, 36.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strict Absolute DER for dataset: 31.60%\n",
      "\n",
      "Processing cam (Medical Domain)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 9/9 [00:00<00:00, 31.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strict Absolute DER for dataset: 34.51%\n",
      "\n",
      "Summary of DER and accumulated metrics across all models:\n",
      "        model  Absolute DER  False Alarm Rate  Missed Detection Rate  \\\n",
      "0  assemblyai      0.254549          0.155119               0.052792   \n",
      "1    deepgram      0.291206          0.144983               0.073664   \n",
      "2  sortformer      0.395750          0.144836               0.195813   \n",
      "3    pyannote      0.315981          0.195521               0.045467   \n",
      "4      soniox      0.420960          0.259506               0.002067   \n",
      "5      reverb      0.315981          0.195521               0.045467   \n",
      "6         cam      0.345086          0.166428               0.085114   \n",
      "\n",
      "   Confusion Rate  \n",
      "0        0.046638  \n",
      "1        0.072558  \n",
      "2        0.055101  \n",
      "3        0.074993  \n",
      "4        0.159388  \n",
      "5        0.074993  \n",
      "6        0.093544  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for name, df in datasets.items():\n",
    "    print(f\"\\nProcessing {name} (Medical Domain)...\")\n",
    "    der_df, abs_der, fa_rate, miss_rate, confusion_rate = compute_strict_der_for_dataset(df[df['domain'] == 'OSCE-Doctor-Patient'])\n",
    "    der_results_all[name] = der_df\n",
    "    abs_ders_all[name] = {\n",
    "        'Absolute DER': abs_der,\n",
    "        'False Alarm Rate': fa_rate,\n",
    "        'Missed Detection Rate': miss_rate,\n",
    "        'Confusion Rate': confusion_rate\n",
    "    }\n",
    "\n",
    "# Create a DataFrame to summarize the results\n",
    "strict_der_df_all = pd.DataFrame.from_dict(abs_ders_all, orient='index').reset_index().rename(columns={'index': 'model'})\n",
    "\n",
    "print(\"\\nSummary of DER and accumulated metrics across all models:\")\n",
    "print(strict_der_df_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "233c178b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing assemblyai (Non-Medical Domain)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/13 [00:00<?, ?it/s]/home/kelechi/miniconda3/envs/nemo310/lib/python3.10/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n",
      "Processing: 100%|██████████| 13/13 [00:00<00:00, 88.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strict Absolute DER for dataset: 7.06%\n",
      "\n",
      "Processing deepgram (Non-Medical Domain)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 13/13 [00:00<00:00, 85.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strict Absolute DER for dataset: 9.82%\n",
      "\n",
      "Processing sortformer (Non-Medical Domain)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 13/13 [00:00<00:00, 15.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strict Absolute DER for dataset: 24.92%\n",
      "\n",
      "Processing pyannote (Non-Medical Domain)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 13/13 [00:00<00:00, 99.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strict Absolute DER for dataset: 20.96%\n",
      "\n",
      "Processing soniox (Non-Medical Domain)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 13/13 [00:00<00:00, 138.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strict Absolute DER for dataset: 15.28%\n",
      "\n",
      "Processing reverb (Non-Medical Domain)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 13/13 [00:00<00:00, 99.86it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strict Absolute DER for dataset: 20.96%\n",
      "\n",
      "Processing cam (Non-Medical Domain)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 13/13 [00:00<00:00, 67.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strict Absolute DER for dataset: 16.73%\n",
      "\n",
      "Summary of DER and accumulated metrics across all models:\n",
      "        model  Absolute DER  False Alarm Rate  Missed Detection Rate  \\\n",
      "0  assemblyai      0.070584          0.026065               0.008120   \n",
      "1    deepgram      0.098169          0.026755               0.022876   \n",
      "2  sortformer      0.249245          0.059675               0.131201   \n",
      "3    pyannote      0.209578          0.156860               0.008787   \n",
      "4      soniox      0.152786          0.035866               0.000614   \n",
      "5      reverb      0.209578          0.156860               0.008787   \n",
      "6         cam      0.167285          0.029676               0.040566   \n",
      "\n",
      "   Confusion Rate  \n",
      "0        0.036399  \n",
      "1        0.048538  \n",
      "2        0.058369  \n",
      "3        0.043931  \n",
      "4        0.116305  \n",
      "5        0.043931  \n",
      "6        0.097043  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for name, df in datasets.items():\n",
    "    print(f\"\\nProcessing {name} (Non-Medical Domain)...\")\n",
    "    der_df, abs_der, fa_rate, miss_rate, confusion_rate = compute_strict_der_for_dataset(df[df['domain'] == 'Chit-Chat-NG'])\n",
    "    der_results_all[name] = der_df\n",
    "    abs_ders_all[name] = {\n",
    "        'Absolute DER': abs_der,\n",
    "        'False Alarm Rate': fa_rate,\n",
    "        'Missed Detection Rate': miss_rate,\n",
    "        'Confusion Rate': confusion_rate\n",
    "    }\n",
    "\n",
    "# Create a DataFrame to summarize the results\n",
    "strict_der_df_all = pd.DataFrame.from_dict(abs_ders_all, orient='index').reset_index().rename(columns={'index': 'model'})\n",
    "\n",
    "print(\"\\nSummary of DER and accumulated metrics across all models:\")\n",
    "print(strict_der_df_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f004f034",
   "metadata": {},
   "source": [
    "## Calculate Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ec5a477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing AssemblyAI for Overlap:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelechi/miniconda3/envs/nemo310/lib/python3.10/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
      "  warnings.warn(\n",
      "Processing AssemblyAI for Overlap: 100%|██████████| 30/30 [00:00<00:00, 335.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap Detection Error Rate: 21.32%\n",
      "Overlap Detection Error Rate: 22.52%\n",
      "Overlap Detection Error Rate: 16.04%\n",
      "Overlap Detection Error Rate: 27.92%\n",
      "Overlap Detection Error Rate: 23.69%\n",
      "Overlap Detection Error Rate: 23.13%\n",
      "Overlap Detection Error Rate: 17.04%\n",
      "Overlap Detection Error Rate: 17.44%\n",
      "Overlap Detection Error Rate: 19.53%\n",
      "Overlap Detection Error Rate: 0.97%\n",
      "Overlap Detection Error Rate: 5.88%\n",
      "Overlap Detection Error Rate: 3.25%\n",
      "Overlap Detection Error Rate: 2.61%\n",
      "Overlap Detection Error Rate: 2.37%\n",
      "Overlap Detection Error Rate: 6.57%\n",
      "Overlap Detection Error Rate: 6.88%\n",
      "Overlap Detection Error Rate: 1.26%\n",
      "Overlap Detection Error Rate: 4.54%\n",
      "Overlap Detection Error Rate: 3.35%\n",
      "Overlap Detection Error Rate: 1.40%\n",
      "Overlap Detection Error Rate: 3.06%\n",
      "Overlap Detection Error Rate: 4.69%\n",
      "Overlap Detection Error Rate: 2.69%\n",
      "Overlap Detection Error Rate: 2.99%\n",
      "Overlap Detection Error Rate: 6.20%\n",
      "Overlap Detection Error Rate: 1.73%\n",
      "Overlap Detection Error Rate: 7.46%\n",
      "Overlap Detection Error Rate: 14.96%\n",
      "Overlap Detection Error Rate: 3.29%\n",
      "Overlap Detection Error Rate: 48.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pyannote.metrics.detection import DetectionErrorRate\n",
    "from pyannote.core import Annotation, Segment\n",
    "\n",
    "def compute_overlap_detection(ref_annotation, pred_annotation):\n",
    "    # Initialize DetectionErrorRate with focus on overlap\n",
    "    overlap_metric = DetectionErrorRate(focus=\"overlap\")\n",
    "    \n",
    "    # Compute overlap detection error\n",
    "    error_rate = overlap_metric(ref_annotation, pred_annotation)\n",
    "    print(f\"Overlap Detection Error Rate: {100 * error_rate:.2f}%\")\n",
    "    return error_rate\n",
    "\n",
    "# apply to assemblyai dataset\n",
    "for i, row in tqdm(assemblyai.iterrows(), total=len(assemblyai), desc=\"Processing AssemblyAI for Overlap\"):\n",
    "    if not (isinstance(row['ref_segments'], (str, list)) and isinstance(row['pred_segments'], (str, list))):\n",
    "        continue\n",
    "    ref_annotation = create_pyannote_annotation(eval(row['ref_segments']) if isinstance(row['ref_segments'], str) else row['ref_segments'])\n",
    "    pred_annotation = create_pyannote_annotation(eval(row['pred_segments']) if isinstance(row['pred_segments'], str) else row['pred_segments'])\n",
    "    error_rate = compute_overlap_detection(ref_annotation, pred_annotation)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf6739b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding Overlaps in AssemblyAI:   0%|          | 0/30 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Annotation' object has no attribute 'overlap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m ref_annotation \u001b[38;5;241m=\u001b[39m create_pyannote_annotation(\u001b[38;5;28meval\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref_segments\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref_segments\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref_segments\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     14\u001b[0m pred_annotation \u001b[38;5;241m=\u001b[39m create_pyannote_annotation(\u001b[38;5;28meval\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_segments\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_segments\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_segments\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 16\u001b[0m ref_overlaps \u001b[38;5;241m=\u001b[39m \u001b[43mget_overlapping_segments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_annotation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m pred_overlaps \u001b[38;5;241m=\u001b[39m get_overlapping_segments(pred_annotation)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m, in \u001b[0;36mget_overlapping_segments\u001b[0;34m(annotation)\u001b[0m\n\u001b[1;32m      2\u001b[0m overlaps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m segment, track \u001b[38;5;129;01min\u001b[39;00m annotation\u001b[38;5;241m.\u001b[39mitertracks(yield_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Check if the segment overlaps with any other segment\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mannotation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverlap\u001b[49m(segment):\n\u001b[1;32m      6\u001b[0m         overlaps\u001b[38;5;241m.\u001b[39mappend(segment)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m overlaps\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Annotation' object has no attribute 'overlap'"
     ]
    }
   ],
   "source": [
    "def get_overlapping_segments(annotation):\n",
    "    overlaps = []\n",
    "    for segment, track in annotation.itertracks(yield_label=False):\n",
    "        # Check if the segment overlaps with any other segment\n",
    "        if annotation.overlap(segment):\n",
    "            overlaps.append(segment)\n",
    "    return overlaps\n",
    "\n",
    "# Example usage with assemblyai dataset\n",
    "for i, row in tqdm(assemblyai.iterrows(), total=len(assemblyai), desc=\"Finding Overlaps in AssemblyAI\"):\n",
    "    if not (isinstance(row['ref_segments'], (str, list)) and isinstance(row['pred_segments'], (str, list))):\n",
    "        continue\n",
    "    ref_annotation = create_pyannote_annotation(eval(row['ref_segments']) if isinstance(row['ref_segments'], str) else row['ref_segments'])\n",
    "    pred_annotation = create_pyannote_annotation(eval(row['pred_segments']) if isinstance(row['pred_segments'], str) else row['pred_segments'])\n",
    "    \n",
    "    ref_overlaps = get_overlapping_segments(ref_annotation)\n",
    "    pred_overlaps = get_overlapping_segments(pred_annotation)\n",
    "    \n",
    "    print(f\"Audio ID: {row['audio_id']}\")\n",
    "    print(f\"Reference Overlapping Segments: {ref_overlaps}\")\n",
    "    print(f\"Predicted Overlapping Segments: {pred_overlaps}\")\n",
    "    print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
